<workflow-app xmlns='uri:oozie:workflow:0.4' name='process-solar-accounts'>

    <start to='sqoopAccounts'/>

    <action name="sqoopAccounts">
        <sqoop xmlns="uri:oozie:sqoop-action:0.2">
            <job-tracker>${jobTracker}</job-tracker>
            <name-node>${nameNode}</name-node>

            <configuration>
                <!-- Setting this property to work around a possible bug in sqoop when password file is used -->
                <!-- See http://stackoverflow.com/questions/19735672/sqoop-password-file-option-closes-the-filesystem-reference-causing-java-io-ioe -->
                <property>
                    <name>fs.hdfs.impl.disable.cache</name>
                    <value>true</value>
                </property>
            </configuration>

            <arg>job</arg>
            <arg>--meta-connect</arg>
            <arg>${sqoopMetastoreUrl}</arg>
            <arg>--exec</arg>
            <arg>sqoopAccounts</arg>
        </sqoop>
        <ok to="normalizeData"/>
        <error to="kill"/>
    </action>

    <action name="normalizeData">
        <pig>
            <job-tracker>${jobTracker}</job-tracker>
            <name-node>${nameNode}</name-node>

            <prepare>
                <delete path="${weather}"/>
                <delete path="${sunrise_sunset}"/>
                <delete path="${solar}"/>
            </prepare>

            <configuration>
                <property>
                    <name>udf.import.list</name>
                    <value>
                        com.jeklsoft.hadoop.piggybank
                    </value>
                </property>
            </configuration>

            <script>../pig/normalize.pig</script>

            <argument>-param</argument>
            <argument>raw_solar=${raw_solar}</argument>

            <argument>-param</argument>
            <argument>raw_sunrise_sunset=${raw_sunrise_sunset}</argument>

            <argument>-param</argument>
            <argument>raw_wx=${raw_wx}</argument>

            <argument>-param</argument>
            <argument>solar=${solar}</argument>

            <argument>-param</argument>
            <argument>sunrise_sunset=${sunrise_sunset}</argument>

            <argument>-param</argument>
            <argument>weather=${weather}</argument>
        </pig>
        <ok to="mergeAndFilterData"/>
        <error to="kill"/>
    </action>

    <action name="mergeAndFilterData">
        <pig>
            <job-tracker>${jobTracker}</job-tracker>
            <name-node>${nameNode}</name-node>

            <prepare>
                <delete path="${solar_and_wx}"/>
                <delete path="${solar_and_wx_grouped_and_sorted_by_day}"/>
            </prepare>

            <configuration>
                <property>
                    <name>udf.import.list</name>
                    <value>
                        com.jeklsoft.hadoop.piggybank
                    </value>
                </property>
            </configuration>

            <script>../pig/mergeAndFilter.pig</script>

            <argument>-param</argument>
            <argument>accounts=${accounts}</argument>

            <argument>-param</argument>
            <argument>solar=${solar}</argument>

            <argument>-param</argument>
            <argument>weather=${weather}</argument>

            <argument>-param</argument>
            <argument>solar_and_wx=${solar_and_wx}</argument>

            <argument>-param</argument>
            <argument>solar_and_wx_grouped_and_sorted_by_day=${solar_and_wx_grouped_and_sorted_by_day}</argument>
        </pig>
        <ok to="createImageFiles"/>
        <error to="kill"/>
    </action>

    <action name="createImageFiles">
        <map-reduce>
            <job-tracker>${jobTracker}</job-tracker>
            <name-node>${nameNode}</name-node>

            <prepare>
                <delete path="${daily}"/>
            </prepare>

            <configuration>
                <property>
                    <name>mapred.mapper.class</name>
                    <value>com.jeklsoft.hadoop.mr.BasicMapper</value>
                </property>
                <property>
                    <name>mapred.combiner.class</name>
                    <value>com.jeklsoft.hadoop.mr.SolarReadingReducer</value>
                </property>
                <property>
                    <name>mapred.reducer.class</name>
                    <value>com.jeklsoft.hadoop.mr.ImageGeneratingReducer</value>
                </property>
                <property>
                    <name>mapred.input.key.class</name>
                    <value>org.apache.hadoop.io.Text</value>
                </property>
                <property>
                    <name>mapred.input.value.class</name>
                    <value>org.apache.hadoop.io.Text</value>
                </property>
                <property>
                    <name>mapred.input.format.class</name>
                    <value>org.apache.hadoop.mapred.TextInputFormat</value>
                </property>
                <property>
                    <name>mapred.output.key.class</name>
                    <value>org.apache.hadoop.io.Text</value>
                </property>
                <property>
                    <name>mapred.output.value.class</name>
                    <value>org.apache.hadoop.io.Text</value>
                </property>
                <property>
                    <name>mapred.output.format.class</name>
                    <value>com.jeklsoft.hadoop.mr.DailyOutputFormat</value>
                </property>

                <property>
                    <name>mapred.input.dir</name>
                    <value>${solar_and_wx}</value>
                </property>
                <property>
                    <name>mapred.output.dir</name>
                    <value>${daily}</value>
                </property>
            </configuration>
        </map-reduce>
        <ok to="generateSalesData"/>
        <error to="kill"/>
    </action>

    <fork name="generateSalesData">
        <path start="generateSolarAndNonSolarUsers"/>
        <path start="generateContactList"/>
    </fork>

    <action name="generateSolarAndNonSolarUsers">
        <hive xmlns="uri:oozie:hive-action:0.2">
            <job-tracker>${jobTracker}</job-tracker>
            <name-node>${nameNode}</name-node>

            <script>../hive/getSolarAndNonSolarAccounts.hql</script>

            <param>resourcePath=${resourcePath}</param>
            <param>resultPath=${resultPath}</param>
        </hive>
        <ok to="salesDataGenerated"/>
        <error to="kill"/>
    </action>

    <action name="generateContactList">
        <hive xmlns="uri:oozie:hive-action:0.2">
            <job-tracker>${jobTracker}</job-tracker>
            <name-node>${nameNode}</name-node>

            <script>../hive/getAccountsToContact.hql</script>

            <param>resourcePath=${resourcePath}</param>
            <param>resultPath=${resultPath}</param>
        </hive>
        <ok to="salesDataGenerated"/>
        <error to="kill"/>
    </action>

    <join name="salesDataGenerated" to="end"/>

    <kill name="kill">
        <message>${wf:lastErrorNode()} failed, error message[${wf:errorMessage(wf:lastErrorNode())}]</message>
    </kill>

    <end name='end'/>

</workflow-app>